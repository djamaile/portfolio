"use strict";(self.webpackChunkdjamaile_dev=self.webpackChunkdjamaile_dev||[]).push([[492],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=a.createContext({}),i=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=i(e.components);return a.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=i(n),d=o,h=m["".concat(p,".").concat(d)]||m[d]||c[d]||r;return n?a.createElement(h,l(l({ref:t},u),{},{components:n})):a.createElement(h,l({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,l=new Array(r);l[0]=m;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:o,l[1]=s;for(var i=2;i<r;i++)l[i]=n[i];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4308:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return p},metadata:function(){return i},assets:function(){return u},toc:function(){return c},default:function(){return d}});var a=n(7462),o=n(3366),r=(n(7294),n(3905)),l=["components"],s={slug:"using-keda-and-prometheus",title:"Using KEDA and Prometheus to scale your k8s workloads",author:"Djamaile Rahamat",author_title:"Software Engineer",author_url:"https://github.com/djamaile",author_image_url:"https://avatars.githubusercontent.com/u/15789670?v=4",tags:["keda","prometheus","kubernetes"]},p=void 0,i={permalink:"/blog/using-keda-and-prometheus",editUrl:"https://github.com/djamaile/portfolio/edit/master/blog/2021-10-31-using-keda-and-prometheus-to-scale.md",source:"@site/blog/2021-10-31-using-keda-and-prometheus-to-scale.md",title:"Using KEDA and Prometheus to scale your k8s workloads",description:"These days, everyone and their grandma are using Kubernetes and one important",date:"2021-10-31T00:00:00.000Z",formattedDate:"October 31, 2021",tags:[{label:"keda",permalink:"/blog/tags/keda"},{label:"prometheus",permalink:"/blog/tags/prometheus"},{label:"kubernetes",permalink:"/blog/tags/kubernetes"}],readingTime:7.365,truncated:!0,authors:[{name:"Djamaile Rahamat",title:"Software Engineer",url:"https://github.com/djamaile",imageURL:"https://avatars.githubusercontent.com/u/15789670?v=4"}],prevItem:{title:"Leaving Bol.com",permalink:"/blog/leaving-bol-dot-com"},nextItem:{title:"How to setup husky, eslint and prettier within minutes",permalink:"/blog/how-to-setup-husky-eslint-prettier"}},u={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",children:[]},{value:"Starting up",id:"starting-up",children:[]},{value:"Deploying Keda",id:"deploying-keda",children:[]},{value:"Creating the application (Optional)",id:"creating-the-application-optional",children:[]},{value:"Running the application",id:"running-the-application",children:[]},{value:"Scaling the application",id:"scaling-the-application",children:[]}],m={toc:c};function d(e){var t=e.components,n=(0,o.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"These days, everyone and their grandma are using Kubernetes and one important\naspect of Kubernetes is scaling your workloads. With KEDA, it is extremely\nsimple to scale your workloads! Let\u2019s have a look."),(0,r.kt)("p",null,"repository: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/djamaile/keda-demo"},"https://github.com/djamaile/keda-demo")),(0,r.kt)("h3",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"Straight from the website of ",(0,r.kt)("a",{parentName:"p",href:"https://keda.sh"},"KEDA"),":"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"KEDA is a Kubernetes-based Event Driven\nAutoscaler. With KEDA, you can drive the scaling of any container in Kubernetes\nbased on the number of events needing to be processed.")),(0,r.kt)("p",null,"KEDA provides many 'triggers' on which your application can scale on. For\nexample, Prometheus, PubSub, Postgres and many more. In this blog post we will\nfocus on Prometheus."),(0,r.kt)("h3",{id:"starting-up"},"Starting up"),(0,r.kt)("p",null,"First let's spin up a cluster! I am using ",(0,r.kt)("a",{parentName:"p",href:"https://kind.sigs.k8s.io/"},"kind")," but\nyou are free to use minikube if you prefer that :)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kind create cluster\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Create the namespace")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl create ns keda-demo\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Switch to the namespace")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl config set-context --current --namespace=keda-demo\n")),(0,r.kt)("p",null,"If the cluster is spun up, we can start deploying our Prometheus. For this, I\nhave already written a prometheus manifest so you won\u2019t have to do it."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"prometheus.yaml")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n  - apiGroups: [""]\n    resources:\n      - services\n    verbs: ["get", "list", "watch"]\n  - nonResourceURLs: ["/metrics"]\n    verbs: ["get"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: keda-demo\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n  - kind: ServiceAccount\n    name: keda-demo\n    namespace: keda-demo\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prom-conf\n  labels:\n    name: prom-conf\ndata:\n  prometheus.yml: |-\n    global:\n      scrape_interval: 5s\n      evaluation_interval: 5s\n    scrape_configs:\n      - job_name: \'go-prom-job\'\n        kubernetes_sd_configs:\n        - role: service\n        relabel_configs:\n        - source_labels: [__meta_kubernetes_service_label_run]\n          regex: go-prom-app-service\n          action: keep\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      serviceAccountName: keda-demo\n      containers:\n        - name: prometheus\n          image: prom/prometheus\n          args:\n            - "--config.file=/etc/prometheus/prometheus.yml"\n            - "--storage.tsdb.path=/prometheus/"\n          ports:\n            - containerPort: 9090\n          volumeMounts:\n            - name: prometheus-config-volume\n              mountPath: /etc/prometheus/\n            - name: prometheus-storage-volume\n              mountPath: /prometheus/\n      volumes:\n        - name: prometheus-config-volume\n          configMap:\n            defaultMode: 420\n            name: prom-conf\n\n        - name: prometheus-storage-volume\n          emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-service\nspec:\n  ports:\n    - port: 9090\n      protocol: TCP\n  selector:\n    app: prometheus-server\n')),(0,r.kt)("p",null,"The Prometheus manifest is really simple. Just a Prometheus workload with a\nclusterrole and a clusterrolebinding. Don't forget to apply the manifest:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl apply -f prometheus.yaml\n")),(0,r.kt)("p",null,"Once the pod is up and running, let's see if it also works:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl port-forward svc/prometheus-service 9090\n")),(0,r.kt)("p",null,"Now visit ",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:9090")," and you should see the user interface of\nPrometheus."),(0,r.kt)("h3",{id:"deploying-keda"},"Deploying Keda"),(0,r.kt)("p",null,"We can now deploy the KEDA operator. KEDA provides multiple ways to deploy their\noperator, but for now we will use the k8s manifest."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl apply -f https://github.com/kedacore/keda/releases/download/v2.4.0/keda-2.4.0.yaml\n")),(0,r.kt)("p",null,"Now there should be two pods in the namespace ",(0,r.kt)("inlineCode",{parentName:"p"},"keda")," you can check it with the\nfollowing command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl get pods -n keda\n")),(0,r.kt)("p",null,"As you can see there are two pods being spinned up:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"on \ud83e\udd20 kind-kind (keda) Desktop/projects/keda-prometheus \u2601\ufe0f  default\n\ud83d\udd59[ 07:35:40 ] \u276f kubectl get pods                                                         335ms\nNAME                                      READY   STATUS              RESTARTS   AGE\nkeda-metrics-apiserver-66b8c68649-2mwf8   0/1     ContainerCreating   0          5s\nkeda-operator-574c6d4769-q9mlc            0/1     ContainerCreating   0          5s\n")),(0,r.kt)("p",null,"The metrics-apiserver exposes data to the Horizontal Pod Autoscaler, which gets\nconsumed by a deployment. The operator pod activates Kubernetes deployments to\nscale to and from zero on no events."),(0,r.kt)("h3",{id:"creating-the-application-optional"},"Creating the application (Optional)"),(0,r.kt)("p",null,"The application is a simple go application that increments the metric\n",(0,r.kt)("inlineCode",{parentName:"p"},"http_requests")," when you visit it. This section is optional because you are also\nfree to use my docker image."),(0,r.kt)("p",null,"in your folder execute the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"go mod init github.com/djamaile/keda-demo\n")),(0,r.kt)("p",null,"Then in your ",(0,r.kt)("inlineCode",{parentName:"p"},"main.go")," you can put in the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-go"},'package main\n\nimport (\n    "fmt"\n    "log"\n    "net/http"\n\n    "github.com/prometheus/client_golang/prometheus"\n    "github.com/prometheus/client_golang/prometheus/promhttp"\n)\n\ntype Labels map[string]string\n\nvar (\n    httpRequestsCounter = prometheus.NewCounter(prometheus.CounterOpts{\n        Name: "http_requests",\n        Help: "number of http requests",\n    })\n)\n\nfunc init() {\n    // Metrics have to be registered to be exposed:\n    prometheus.MustRegister(httpRequestsCounter)\n}\n\nfunc main() {\n    http.Handle("/metrics", promhttp.Handler())\n    http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {\n        defer httpRequestsCounter.Inc()\n        fmt.Fprintf(w, "Hello, you\'ve requested: %s\\n", r.URL.Path)\n    })\n    log.Fatal(http.ListenAndServe(":8080", nil))\n}\n')),(0,r.kt)("p",null,"Now build the go application with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ go mod tidy\n")),(0,r.kt)("p",null,"Let's then make a simple ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," for it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-dockerfile"},'FROM golang as build-stage\n\nCOPY go.mod /\nCOPY go.sum /\nCOPY main.go /\nRUN cd / && CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o go-prom-app\n\nFROM alpine\nCOPY --from=build-stage /go-prom-app /\nEXPOSE 8080\nCMD ["/go-prom-app"]\n')),(0,r.kt)("p",null,"Only thing left is to build and push the image:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ docker build -t <your_username>/keda .\n$ docker push <your_username>/keda\n")),(0,r.kt)("h3",{id:"running-the-application"},"Running the application"),(0,r.kt)("p",null,"If you don\u2019t have a Docker account or don\u2019t want to use it, that\u2019s fine. You can\nuse my docker image! Let\u2019s get our go application running in our cluster, for\nthat we need some k8s manifests. Not to worry because I already wrote them:"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"go-deployment.yaml")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-prom-app\n  namespace: keda-demo\nspec:\n  selector:\n    matchLabels:\n      app: go-prom-app\n  template:\n    metadata:\n      labels:\n        app: go-prom-app\n    spec:\n      containers:\n        - name: go-prom-app\n          image: djam97/keda\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-prom-app-service\n  namespace: keda-demo\n  labels:\n    run: go-prom-app-service\nspec:\n  ports:\n    - port: 8080\n      protocol: TCP\n  selector:\n    app: go-prom-app\n")),(0,r.kt)("p",null,"You can replace the image name with your own image if you prefer that.\nLet's apply the manifest:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl apply -f go-deployment.yaml\n")),(0,r.kt)("p",null,"If the pod is up verify if it is working"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl port-forward svc/go-prom-app-service 8080\n")),(0,r.kt)("p",null,"If you visit ",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:8080")," you should see ",(0,r.kt)("inlineCode",{parentName:"p"},"Hello, you've requested: /"),"."),(0,r.kt)("h3",{id:"scaling-the-application"},"Scaling the application"),(0,r.kt)("p",null,"Now that we have our go application up we can write a manifest that will scale\nour application. Keda offers many triggers that can scale our application, but\nof course we will use the ",(0,r.kt)("a",{parentName:"p",href:"https://keda.sh/docs/2.4/scalers/prometheus/"},"Prometheus\ntrigger"),"."),(0,r.kt)("p",null,"In a new file called scaled-object.yaml add the following content:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: keda.sh/v1alpha1\n# Custom CRD provisioned by the Keda operator\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\nspec:\n  scaleTargetRef:\n    # target our deployment\n    name: go-prom-app\n  # Interval to when to query Prometheus\n  pollingInterval: 15\n  # The period to wait after the last trigger reported active\n  # before scaling the deployment back to 1\n  cooldownPeriod: 30\n  # min replicas keda will scale to\n  # if you have an app that has an dependency on pubsub\n  # this would be a good use case to set it to zero\n  # why keep your app running if your topic has no messages?\n  minReplicaCount: 1\n  # max replicas keda will scale to\n  maxReplicaCount: 20\n  advanced:\n    # HPA config\n    # Read about it here: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n    horizontalPodAutoscalerConfig:\n      behavior:\n        scaleDown:\n          stabilizationWindowSeconds: 30\n          policies:\n            - type: Percent\n              value: 50\n              periodSeconds: 30\n        scaleUp:\n          stabilizationWindowSeconds: 0\n          policies:\n            - type: Percent\n              value: 50\n              periodSeconds: 10\n  triggers:\n    - type: prometheus\n      metadata:\n        # address where keda can reach our prometheus on\n        serverAddress: http://prometheus-service.keda-demo.svc.cluster.local:9090\n        # metric on what we want to scale\n        metricName: http_requests_total\n        # if treshold is reached then Keda will scale our deployment\n        threshold: "100"\n        query: sum(rate(http_requests[1m]))\n')),(0,r.kt)("p",null,"Read the yaml manifest and it\u2019s comments to understand what is going on. One\nimportant note as well is in\n",(0,r.kt)("inlineCode",{parentName:"p"},"advanced.horizontalPodAutoscalerConfig.scaleUp.policies")," you can see I have\nspecified 50%, that means our pod will scale up with 50% of it\u2019s current amount\nof pods. ",(0,r.kt)("inlineCode",{parentName:"p"},"1 -> 2 -> 3 -> 5 -> 8 -> 12 -> 18 -> 20")," it will stop at 20 pods because\nthat is the limit we specified."),(0,r.kt)("p",null,"Let's apply the manifest:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl apply -f scaled-object.yaml\n")),(0,r.kt)("p",null,"This will provision an HPA in your namespace which you can check with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl get hpa\n")),(0,r.kt)("p",null,"but because this is a custom CRD you can also query the custom CRD with kubectl:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl get scaledobject.keda.sh/prometheus-scaledobject\n\nNAME                      SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS     AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE\nprometheus-scaledobject   apps/v1.Deployment   go-prom-app       1     20    prometheus                    True    False    False      64s\n")),(0,r.kt)("p",null,"We can see that our ",(0,r.kt)("inlineCode",{parentName:"p"},"prometheus-scaledobject")," is ready so let\u2019s scale our\napplication! Remember our application scales on the metric\n",(0,r.kt)("inlineCode",{parentName:"p"},"http_requests_total"),"\nand our threshold is only 100 so we should be able reach that threshold. For\nthis we can use a simple tool called ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/rakyll/hey"},"hey"),"."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Run the application")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl port-forward svc/go-prom-app-service 8080\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"In another terminal watch the pods")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ kubectl get pods -w -n keda-demo\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Put load on the application (Do this continuously, until there are 20 pods)")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ hey -n 10000 -m GET http://localhost:8080\n")),(0,r.kt)("p",null,"It can take a minute before the application actually starts scaling. After a\nwhile you should have 20 pods up and running! Now let\u2019s also look at the scale\ndown process. Stop putting load on the application and let\u2019s just watch the\npods. This process should go from ",(0,r.kt)("inlineCode",{parentName:"p"},"20 -> 10 -> 5 - > 2 -> 1"),". This is basically\nhow KEDA goes to work!"),(0,r.kt)("p",null,"If you like KEDA please check out their docs for more examples and what type of\ndifferent triggers they provide. Happy auto-scaling!"))}d.isMDXComponent=!0}}]);